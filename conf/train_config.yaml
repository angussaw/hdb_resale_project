files:
  derived_features:
    read_from_source: postgres
    params:
      table_name: "hdb_training_features"
      columns:
        - month
        - year
        - flat_type
        - storey_range
        - floor_area_sqm
        - flat_model
        - lease_age
        - no_of_malls_within_2_km
        - distance_to_nearest_malls
        - no_of_schools_within_2_km
        - distance_to_nearest_schools
        - no_of_parks_within_2_km
        - distance_to_nearest_parks
        - no_of_MRT_stations_within_2_km
        - distance_to_nearest_MRT_stations
        - region
        - resale_price

label_column: resale_price

defaults:
  - override hydra/sweeper: "optuna"
  - override hydra/sweeper/sampler: "tpe"

hydra:
  sweeper:
    sampler:
      seed: 42
    direction: ["minimize"]
    study_name: "random_seed_tuning"
    storage: null
    n_trials: 20
    n_jobs: 1

    params:
      model_params.xgboost.params.max_depth: range(5,15,1)
      model_params.xgboost.params.n_estimators: range(100,500,10)
      model_params.xgboost.params.learning_rate: range(0.1,1.0,0.1)

process_train_data:
  ordinal_encoding:
    - storey_range

  train_test_val_split:
    train_size: 0.7
    test_size: # Split between test and eval, leave blank to only have train, test
    random_state: 42

model_params:
  chosen_model: "xgboost" # ebm, randforest, xgboost

  randforest:
    model_name: "randforest"
    one_hot_encode: True
    scale_data: False
    params:
      random_state: 42
      n_estimators: 100 #Default: 100
      max_depth: 10
      min_samples_leaf: 1 #Default: 1
      min_samples_split: 2 #Default: 2

  ebm:
    model_name: "ebm"
    one_hot_encode: True
    scale_data: False
    params:
      random_state: 42 #Default: 42
      max_bins: 256 #Default: 256
      max_interaction_bins: 32 #Default: 32
      interactions: 10 #Default: 10
      learning_rate: 0.01 #Default: 0.01
      inner_bags: 0 #Default: 0
      outer_bags: 8 #Default: 8
      min_samples_leaf: 2 #Default: 2
      max_leaves: 3 #Default: 3

  xgboost:
    model_name: "xgboost"
    one_hot_encode: True
    scale_data: False
    params:
      max_depth: 4
      n_estimators: 150
      learning_rate: 0.1

mlflow:
  artifact_name: 'testing'
  experiment_name: "testing"
  description: 'testing'
  model_name: testing.joblib
  tags:
    model-architecture: "testing"

evaluator:
  rounding_last_n: 5
  feature_importance_top_n: 5
  no_of_cv_folds: 5 # Leave blank to use train-test or train-val-test split

optimisation_metric: "test_root_mean_squared_error" # metric for optimisation